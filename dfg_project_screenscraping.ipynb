{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fad95a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import relevant libraries\n",
    "#requests for get requests\n",
    "#csv to convert file into csv\n",
    "#re to extract the maximum number of page with a regular expression\n",
    "#BeautifulSoup to scrape data\n",
    "import requests\n",
    "import csv\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#get the url-data to extract the max iterations of first for-loop (the max amount of project-lists via 50 projects)\n",
    "#future work: kinda redundant to scrape the same URL twice (initial_URL and listURL). There should be a better solution\n",
    "initial_URL = \"https://gepris.dfg.de/gepris/OCTOPUS?beginOfFunding=&bewilligungsStatus=&bundesland=DEU%23&context=projekt&einrichtungsart=-1&fachgebiet=%23&findButton=historyCall&gefoerdertIn=&ggsHunderter=0&hitsPerPage=50&index=0&nurProjekteMitAB=false&oldGgsHunderter=0&oldfachgebiet=%23&pemu=%23&task=doKatalog&teilprojekte=true&zk_transferprojekt=false\"\n",
    "website = requests.get(initial_URL)\n",
    "#convert to beautifulsoup-object\n",
    "results = BeautifulSoup(website.content, 'html.parser')\n",
    "#get the number of pages that needs to extracted. \n",
    "#the number that we want is saved as '2.751'. To get rid of the dot and convert it into an integer-value, the numbers are extracted individually as an array, joined and converted to an integer value\n",
    "number_pages_to_extract = int(''.join(re.findall(r'\\d+', results.find(\"span\", {\"id\": \"result-info\"}).find(\"strong\").get_text())))\n",
    "#the keys that we are interested in to extract. Needs the exact name of the possible keys from a project page (Example of a project page: https://gepris.dfg.de/gepris/projekt/268931)\n",
    "keys_to_extract = [\"Fachliche Zuordnung\", \"Förderung\", \"Webseite\", \"DFG-Verfahren\"]\n",
    "#create empty dictionary to fill it with data \n",
    "data = []\n",
    "\n",
    "for x in range(number_pages_to_extract):\n",
    "\n",
    "    listURL = \"https://gepris.dfg.de/gepris/OCTOPUS?beginOfFunding=&bewilligungsStatus=&bundesland=DEU%23&context=projekt&einrichtungsart=-1&fachgebiet=%23&findButton=historyCall&gefoerdertIn=&ggsHunderter=0&hitsPerPage=50&index=\"+str(x*50)+\"&nurProjekteMitAB=false&oldGgsHunderter=0&oldfachgebiet=%23&pemu=%23&task=doKatalog&teilprojekte=true&zk_transferprojekt=false\"\n",
    "    website = requests.get(listURL)\n",
    "    results = BeautifulSoup(website.content, 'html.parser')\n",
    "\n",
    "    #get GEPRIS Project ID to get the URL of a specific project-page of the project-list\n",
    "    projects = results.find_all(\"div\", class_=\"results\")\n",
    "\n",
    "    for project in projects:\n",
    "        URL = \"https://gepris.dfg.de\" + project.find('a').get('href')\n",
    "        print(str(x) + \" : \" + URL)\n",
    "\n",
    "        website = requests.get(URL)\n",
    "        results = BeautifulSoup(website.content, 'html.parser')\n",
    "\n",
    "        #array that contains html-code for every value that can be extracted from a project-page \n",
    "        values = results.find_all('span', class_='value')\n",
    "        #array that contains all names (keys) that can be extracted from a project-page\n",
    "        names = results.find_all('span', class_='name')\n",
    "        #empty dict to store relevant information about a project\n",
    "        projectdict = {}\n",
    "\n",
    "        projectdict[\"GEPRIS Project ID\"] = URL.rsplit('/', 1)[-1]\n",
    "        # .text to get only the information without html-code\n",
    "        # .join(.split) to get rid of all whitespaces in the information\n",
    "        projectdict[\"Projekttitel\"] = \" \".join(results.find('h1', class_='facelift').text.split())\n",
    "        projectdict[\"Wikidata Description\"] = \"GEPRIS Projekt\"\n",
    "        projectdict[\"DFG-Webseite\"] = URL\n",
    "        \n",
    "        #for every value in a project page\n",
    "        for index, info in enumerate(values):\n",
    "            #get the key out of html-construct\n",
    "            key = \" \".join(names[index].text.split())\n",
    "            #check if the key should be added to dict\n",
    "            if key in keys_to_extract:\n",
    "                #If the string is \"Zur Hompage\" (get to website), don't extract the string, but the href-attribute\n",
    "                if (\" \".join(info.text.split()) == 'Zur Homepage'):\n",
    "                    value = info.find('a').get('href')\n",
    "                #The value of \"Förderung\" contains two pieces of information and needs to be split in \"von\" (start time of funding) and \"bis\" (end time of funding)\n",
    "                #There are three possible outcomes for the value \"Förderung\"\n",
    "                # 1: If the string contains \"bis\", it also contains the starttime and endtime, which can be extracted\n",
    "                # 2: If the string contains \"in\", the start- and endtime are same\n",
    "                # 3: If the string contains \"seit\", there is no endtime inside the string\n",
    "                elif (key == \"Förderung\"):\n",
    "                    foerderung_string = \" \".join(info.text.split())\n",
    "                    if (\"bis\" in foerderung_string):\n",
    "                        projectdict[\"von\"] = foerderung_string[14:18]\n",
    "                        projectdict[\"bis\"] = foerderung_string[-4:]\n",
    "                    elif (\"in\" in foerderung_string):\n",
    "                        projectdict[\"von\"] = foerderung_string[-4:]\n",
    "                        projectdict[\"bis\"] = foerderung_string[-4:]\n",
    "                    elif (\"seit\" in foerderung_string):\n",
    "                        projectdict[\"von\"] = foerderung_string[-4:]\n",
    "                else:\n",
    "                    value = \" \".join(info.text.split())\n",
    "                #to prevent to save the information twice\n",
    "                if key != \"Förderung\":\n",
    "                    projectdict[key] = value\n",
    "        data.append(projectdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd8f228",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#save as csv_file as gepris_projects_wikidata.csv\n",
    "myFile = open('gepris_projects_wikidata.csv', 'w', encoding='utf-8')\n",
    "writer = csv.DictWriter(myFile, lineterminator='\\n', fieldnames=['GEPRIS Project ID', 'Projekttitel', 'Wikidata Description', 'DFG-Webseite', 'Fachliche Zuordnung', 'von', 'bis', 'Webseite', 'DFG-Verfahren'])\n",
    "writer.writeheader()\n",
    "writer.writerows(data)\n",
    "myFile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
